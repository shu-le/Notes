

Linear Alignment: A Closed-form Solution for Aligning Human Preference without Tuning and Feedback

Decoding-time Realignment of Language Models

Max-Min RLHF

AI Alignment with changing and Influenceable Reward Function

Weak-to-strong Preference Optimization: Stealing Reward from Weak Aligned Model

#### Constrained RLHF Form

MAP: Multi-human-value Alignment Palette

One-Shot Safety Alignment for Large Language Models via Optimal Dualization

L3Ms------Lagrange Large Language Models


#### Diversity
Modifying Large Language Model Post-Training for Diverse Creative Writing
Diverse Preference Learning for Capabilities and Alignment
Be More Diverse than the Most Diverse: Optimal Mixtures of Generative Models via Mixture-UCB Bandit Algorithms
An Information-Theoretic Evaluation of Generative Models in Learning Multi-modal Distributions(Renyi Kernel Entropy)
Preserving Diversity in Supervised Fine-Tuning of Large Language Models
Diversity-Rewarded CFG Distillation

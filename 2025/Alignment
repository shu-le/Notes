

Linear Alignment: A Closed-form Solution for Aligning Human Preference without Tuning and Feedback

Decoding-time Realignment of Language Models

Max-Min RLHF

AI Alignment with changing and Influenceable Reward Function

Weak-to-strong Preference Optimization: Stealing Reward from Weak Aligned Model

#### Constrained RLHF Form

MAP: Multi-human-value Alignment Palette

One-Shot Safety Alignment for Large Language Models via Optimal Dualization

L3Ms------Lagrange Large Language Models
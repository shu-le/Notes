## Text-to-image

[Aligning Text-to-Image Diffusion Models with Reward Backpropagation](https://arxiv.org/abs/2310.03739) 5 Oct 23

[FreeU: Free Lunch in Diffusion U-Net](https://arxiv.org/abs/2309.11497) 20 Seq 23

[Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack](https://huggingface.co/papers/2309.15807)



## Control diffusion

[Zero-shot spatial layout conditioning for text-to-image diffusion models](https://arxiv.org/abs/2306.13754)

[Generate Anything Anywhere in Any Scene](https://arxiv.org/abs/2306.17154)

[ConceptLab: Creative Generation using Diffusion Prior Constraints](https://huggingface.co/papers/2308.02669)

[IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models](https://huggingface.co/papers/2308.06721)

[DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models](https://huggingface.co/papers/2309.06933)

Controlling Text-to-Image Diffusion by Orthogonal Finetuning

Continuous Layout Editing of Single Images with Diffusion Models



## Video diffusion

[AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning](https://arxiv.org/abs/2307.04725)

[TokenFlow: Consistent Diffusion Features for Consistent Video Editing](https://huggingface.co/papers/2307.10373)

[Multiscale Video Pretraining for Long-Term Activity Forecasting](https://huggingface.co/papers/2307.12854)

[Dual-Stream Diffusion Net for Text-to-Video Generation](https://huggingface.co/papers/2308.08316)

[DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory](https://huggingface.co/papers/2308.08089)

[VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation](https://huggingface.co/papers/2309.00398)

[MagicProp: Diffusion-based Video Editing via Motion-aware Appearance Propagation](https://huggingface.co/papers/2309.00908)

[Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation](https://huggingface.co/papers/2309.03549)

[LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models](https://huggingface.co/papers/2309.15103)

[VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning](https://huggingface.co/papers/2309.15091)





[Diffusion Generative Inverse Design](https://huggingface.co/papers/2309.02040)

[Generative Image Dynamics](https://huggingface.co/papers/2309.07906)



## 3D

DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model

Viewpoint Textual Inversion: Unleashing Novel View Synthesis with Pretrained 2D Diffusion Models

Zero-1-to-3: Zero-shot One Image to 3D Object

DREAMGAUSSIAN: GENERATIVE GAUSSIAN SPLATTING FOR EFFICIENT 3D CONTENT CREATION

## Other

SEEDS: Emulation of Weather Forecast Ensembles with Diffusion Models

PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment

Generate Anything Anywhere in Any Scene

Benchmarking Large Language Model Capabilities for Conditional Generation

StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners

Wuerstchen: Efficient Pretraining of Text-to-Image Models

DomainStudio: Fine-Tuning Diffusion Models for Domain-Driven Image Generation using Limited Data

Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision

Image Captioners Are Scalable Vision Learners Too

InstructDiffusion: A Generalist Modeling Interface for Vision Tasks

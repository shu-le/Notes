[3D-VLA: A 3D Vision-Language-Action Generative World Model](https://arxiv.org/abs/2403.09631)

Long-CLIP: Unlocking the Long-Text Capability of CLIP



ICML: 

[Dense Reward for Free in Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2402.00782)



Visual-Language Models as Fuzzy Rewards for Reinforcement Learning

video:

[Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning](https://arxiv.org/abs/2402.11435)



NeurIPS

​	RLHF:

​	[Information Theoretic Text-to-Image Alignment](https://arxiv.org/abs/2405.20759)





待看:

[Video Diffusion Models are Training-free Motion Interpreter and Controller](https://arxiv.org/abs/2405.14864)

[Learning Multi-dimensional Human Preference for Text-to-Image Generation](https://arxiv.org/abs/2405.14705)

[Neuroexplicit Diffusion Models for Inpainting of Optical Flow Fields](https://arxiv.org/abs/2405.14599)

[Class-Conditional self-reward mechanism for improved Text-to-Image models](https://arxiv.org/abs/2405.13473)

[Directly Denoising Diffusion Model](https://arxiv.org/abs/2405.13540)

[Direct Preference Optimization With Unobserved Preference Heterogeneity](https://arxiv.org/abs/2405.15065)

[FreeCustom: Tuning-Free Customized Image Generation for Multi-Concept Composition](https://arxiv.org/abs/2405.13870)


[3D-VLA: A 3D Vision-Language-Action Generative World Model](https://arxiv.org/abs/2403.09631)

Long-CLIP: Unlocking the Long-Text Capability of CLIP

[What the DAAM: Interpreting Stable Diffusion Using Cross Attention](https://arxiv.org/abs/2210.04885) 解释sd

[Distributional Preference Alignment of LLMs via Optimal Transport](https://arxiv.org/abs/2406.05882)

[Multi-objective Reinforcement learning from AI Feedback](https://arxiv.org/abs/2406.07295)

[Joint Demonstration and Preference Learning Improves Policy Alignment with Human Feedback](https://arxiv.org/abs/2406.06874)

[Direct Language Model Alignment from Online AI Feedback](https://arxiv.org/abs/2402.04792) 各种算法Online差别不大但都比Offline好

[DPO Meets PPO: Reinforced Token Optimization for RLHF](https://arxiv.org/pdf/2404.18922)

[Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts](https://arxiv.org/abs/2406.12845)

[Understanding the performance gap between online and offline alignment algorithms](https://arxiv.org/abs/2405.08448)

[Beyond One-Preference-Fits-All Alignment: **Multi-Objective Direct Preference Optimization**](https://arxiv.org/abs/2310.03708)



dataset:

​	[MantisScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation](https://arxiv.org/abs/2406.15252)



ICML24: 

[Dense Reward for Free in Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2402.00782)

[Generative Active Learning for Long-tailed Instance Segmentation](https://arxiv.org/abs/2406.02435) 

Visual-Language Models as Fuzzy Rewards for Reinforcement Learning

[Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint](https://arxiv.org/abs/2312.11456)

[Token-level Direct Preference Optimization](https://arxiv.org/abs/2404.11999)

[Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences](https://arxiv.org/abs/2403.01857)

[A Dense Reward View on Aligning Text-to-Image Diffusion with Preference](https://arxiv.org/abs/2402.08265)



video:

[Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning](https://arxiv.org/abs/2402.11435)



NeurIPS

​	RLHF:

​	[Information Theoretic Text-to-Image Alignment](https://arxiv.org/abs/2405.20759)

​	Boost Your Own Human Image Generation Model via Direct Preference Optimization with AI Feedback

​	[Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms](https://arxiv.org/abs/2406.02900)

​	[Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step](https://arxiv.org/abs/2406.04314)

​	[Tuning-Free Alignment of Diffusion Models with Direct Noise Optimization](https://arxiv.org/abs/2405.18881)

​	[ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise Optimization](https://arxiv.org/abs/2406.04312)

​	[Margin-aware Preference Optimization for Aligning Diffusion Models without Reference](https://arxiv.org/abs/2406.06424) ORPO

​	[Diffusion-RPO: Aligning Diffusion Models through Relative Preference Optimization](https://arxiv.org/abs/2406.06382)

​		Two insights: preference learning and conditional transport framework, a new downstream task with datasets specifically designed for image preference learning

​	[Online DPO: Online Direct Preference Optimization with Fast-Slow Chasing](https://arxiv.org/abs/2406.05534)

​	[3D-Properties: Identifying Challenges in DPO and Charting a Path Forward](https://arxiv.org/abs/2406.07327)

​	[OPTune: Efficient Online Preference Tuning](https://arxiv.org/abs/2406.07657)

​	[Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms](https://arxiv.org/abs/2406.09397)

​	[Online Bandit Learning with Offline Preference Data](https://arxiv.org/abs/2406.09574)

​	[Bootstrapping Language Models with DPO Implicit Rewards](https://arxiv.org/abs/2406.09760)

​	[SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset](https://arxiv.org/abs/2406.14477)

​	[Aligning Diffusion Models with Noise-Conditioned Perception](https://arxiv.org/abs/2406.17636)



​	Video:

​	[Searching Priors Makes Text-to-Video Synthesis Better](https://arxiv.org/abs/2406.03215)

​	[ShareGPT4Video: Improving Video Understanding and Generation with Better Captions](https://arxiv.org/abs/2406.04325)

​	[VIDEOPHY: Evaluating Physical Commonsense for Video Generation](https://arxiv.org/abs/2406.03520)

​	[Vript: A Video Is Worth Thousands of Words](https://arxiv.org/abs/2406.06040)

​	[**Splatter a Video: Video Gaussian Representation for Versatile Processing**](https://arxiv.org/abs/2406.13870)(或许可以用来reward)

​	[MotionBooth: Motion-Aware Customized Text-to-Video Generation](https://arxiv.org/abs/2406.17758)

​	[Video-Infinity: Distributed Long Video Generation](https://arxiv.org/abs/2406.16260)



​	Image: 

​	[Text-to-Image Rectified Flow as Plug-and-Play Priors](https://arxiv.org/abs/2406.03293)

​	[Adapter-X: A Novel General Parameter-Efficient Fine-Tuning Framework for Vision](https://arxiv.org/abs/2406.03051) (Mixture of Adapters)

​	[Understanding the Impact of Negative Prompts: When and How Do They Take Effect?](https://arxiv.org/abs/2406.02965)

​	[Guiding a Diffusion Model with a Bad Version of Itself](https://arxiv.org/abs/2406.02507)

​	[An Image is Worth 32 Tokens for Reconstruction and Generation](https://arxiv.org/abs/2406.07550)

​	[Ctrl-X: Controlling Structure and Appearance for Text-To-Image Generation Without Guidance](https://arxiv.org/abs/2406.07540)

​	[Interpreting the Weight Space of Customized Diffusion Models](https://arxiv.org/abs/2406.09413)



​	Evaluate and Benchmark:

​	[A-Bench: Are LMMs Masters at Evaluating AI-generated Images?](https://arxiv.org/abs/2406.03070) Benchmark

​	[GAIA: Rethinking Action Quality Assessment for AI-Generated Videos](https://arxiv.org/abs/2406.06087)

​	[Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?](https://arxiv.org/abs/2406.07546)

​	[Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing Reliability,Reproducibility, and Practicality](https://arxiv.org/abs/2406.08845)

​	[TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and Image-to-Video Generation](https://arxiv.org/abs/2406.08656)

​	[VideoGUI: A Benchmark for GUI Automation from Instructional Videos](https://arxiv.org/abs/2406.10227)

​	[GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual Generation](https://arxiv.org/abs/2406.13743)

​	[EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image Models](https://arxiv.org/abs/2406.16562)



​	Gaussian:

​	[Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion](https://arxiv.org/abs/2406.04338)

​	[Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image](https://arxiv.org/abs/2406.04343)

​	[L4GM: Large 4D Gaussian Reconstruction Model](https://arxiv.org/abs/2406.10324)

​	[A3D: Does Diffusion Dream about 3D Alignment?](https://arxiv.org/abs/2406.15020v1) 可能有用

​	

ECCV

​	[Diffusion Soup: Model Merging for Text-to-Image Diffusion Models](https://arxiv.org/abs/2406.08431)



​	[LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation](https://arxiv.org/abs/2402.05054)

​	

CVPR:

​	[InstructRL4Pix: Training Diffusion for Image Editing by Reinforcement Learning](https://arxiv.org/abs/2406.09973)



ICLR 24:

​	[Image Conductor: Precision Control for Interactive Video Synthesis](https://arxiv.org/abs/2406.15339)

​	[Contrastive Preference Learning: Learning from Human Feedback without RL](https://arxiv.org/abs/2310.13639)

​	[Compositional preference models for aligning LMs](https://arxiv.org/abs/2310.13011)

​	**[Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models](https://arxiv.org/abs/2404.01863)** reward model ensemble



待看:

[Video Diffusion Models are Training-free Motion Interpreter and Controller](https://arxiv.org/abs/2405.14864)

[Learning Multi-dimensional Human Preference for Text-to-Image Generation](https://arxiv.org/abs/2405.14705)

[Neuroexplicit Diffusion Models for Inpainting of Optical Flow Fields](https://arxiv.org/abs/2405.14599)

[Class-Conditional self-reward mechanism for improved Text-to-Image models](https://arxiv.org/abs/2405.13473)

[Directly Denoising Diffusion Model](https://arxiv.org/abs/2405.13540)

[Direct Preference Optimization With Unobserved Preference Heterogeneity](https://arxiv.org/abs/2405.15065)

[FreeCustom: Tuning-Free Customized Image Generation for Multi-Concept Composition](https://arxiv.org/abs/2405.13870)

[Geometry-Aware Score Distillation via 3D Consistent Noising and Gradient Consistency Modeling](https://arxiv.org/abs/2406.16695)
